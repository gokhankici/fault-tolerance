#+TITLE: Reading List
#+OPTIONS: toc:t html-postamble:nil num:nil

* Read
** Building a Distributed Fault-Tolerant Key-Value Store

[[http://blog.fourthbit.com/2015/04/12/building-a-distributed-fault-tolerant-key-value-store][link]]
   
- One coordinator per data-center, elected using a paxos-like consensus protocol.

- *Hinted-handoff*: Cassandra has replication of data. When you try to write
  data but a replica is down, the coordinator keeps an extra local copy per
  failed replica, until it's back up.

- *Committing a write*: 
  1. Persistent memory (disk) in a commit log
  2. In-memory memtable

- Since any node can act as a request coordinator, it is necessary to keep all
  nodes informed at all times of the other nodes in the ring. Such problem is
  addressed by the so-called *membership protocol*.

- *SWIM* (Scalable Weakly-consistent Infection-style Process Group Membership)

- *Eventual consistency*: If no new updates are made, eventually all accesses to
  that item will return the last updated value. Using *read repair*, the
  correction is done when a read finds an inconsistent

- *Always writable*: Write is like appending data with a timestamp into a file.
  The burden of data consistency is brought to read time.

- *Stabilization*: When a node fails, =key->node= mapping breaks for some keys.
  As it fails, there will be n-1 nodes for key distribution, meaning that many
  keys will suddenly have a different set of nodes assigned.
  
  A possible implementation would be iterating over all keys in a node,
  detecting if the key still belongs to the node and if that's not the case,
  delete it from local storage after sending it to all proper replicas.

** Eventual Consistency

[[https://distributedthoughts.wordpress.com/2013/09/08/eventual-consistency/][link]]

#+BEGIN_SRC
1. write(7, "A")
2. write(7, "B")
3. read(7)
4. read(7)
#+END_SRC

in an eventual consistent db, all four results are legal (both lines 3 & 4 can
return "A" or "B"). For example, line 3 can return "B" and line 4 can return
"A", it can /disappear/.

*Why?* Read/write operations can be implemented at a lower latency than strongly
consistent databases.

Requiring low latency means that if a machine receives a read operation, it
should answer that read operation *immediately*. In other words, if it receives a
read operation it *canâ€™t contact another server* before responding.

** Understanding Paxos

[[https://distributedthoughts.wordpress.com/2013/09/22/understanding-paxos-part-1/][link]]

Possible *API* formalization:
  * =add(item)= : tries to add item to the log, and returns the index (or -1)
  * =get(index)= : returns the item at the given index (or =null=)
  * an event triggered each time an item is added, providing the item and its
    index as parameters

When you have only 2 nodes, due to the [[http://en.wikipedia.org/wiki/Split-brain_(computing)][split brain]] problem, you can't store data
in a fault resilient manner. *Paxos ensures that* no matter what if data is
written, it will eventually propagato to all the nodes and there won't be any
nodes that think some index contains different values (conflicting log records).

*Paxos assumes that*:
  * the content of messages aren't changed after they're sent
  * nodes run the algorithm as instructed (no bugs, hackers, etc.)
  * nodes have persistent storage

*Paxos ensures "safety" even if*:
  * messages are dropped or duplicated
  * some (or all) of the nodes crash or restart

#+BEGIN_QUOTE
/Paxos always guarantees "safety", while ensuring "liveness" if a majority of the
nodes are communicating with each other./
#+END_QUOTE
  
Paxos has *three main blocks*:
  1. Leader election protocol - to decide which node is running the show
  2. Consensus on a single log entry (called *Synod*) - preserves "safety"
  3. A protocol for managing the entire log - what entries should be added

A *bird's eye view* of the algorithm:
  * continuously (every X seconds) run the leader election algorithm, so all
    nodes know who's in charge (Paxos doesn't specify which algorithm to use)
  * when the leader receives a request to add a log item =ITEM=, it selects the
    next empty log index =INDEX=, and initiates a consensus on adding
    =(INDEX,ITEM)= to the log.
  * when a consensus is initiated to add =(INDEX,ITEM)=, all nodes participate
    in it, eventually agreeing that item =ITEM= was added to the log at index
    =INDEX=.

If *more than one leader* is elected, and two different leaders initiate a
consensus on =(INDEX,ITEM1)= and =(INDEX,ITEM2)=, Synod algorithm will choose
only one of them.

When the leader is elected, it goes over the log and sees if there are any
*holes* in it. Assume that index 17 is empty, the leader would then initiate the
Synod algorithm with the value =(17,EMPTY)=, and the algorithm will reach to a
consensus on the value =(17,EMPTY)= or =(17,VALUE)= for some previously proposed
value.

Once there are *no more holes*, the leader can start processing writes: for each
request to add an item to the log, the leader initiates the consensus algorithm
with =(INDEX,ITEM)= where =INDEX= is the next empty slot.

*To sum up*, Paxos:
1. is a highly resilient algorithm providing an abstraction of a
   distributed log.
2. assumes some mechanism of electing a leader.
3. ensures that data log is consistent, even if there are none or multiple
   leaders.
4. will be able to perform writes, if more than half of the nodes can
   communicate.

** Paxos Made Simple

[[http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf][link]]

*Problem:* A collection of networked processes propose values, we have to pick
one.

*Safety requirements:*
- a single one among the proposed values is chosen
- if no value is proposed, no value should be chosen
- if a value has been chosen, then processes should be able to learn the chosen
  value

*Processes:*
- 3 classes of agents: /proposers/, /acceptors/ and /learners/.
- Agents operate at different speeds, may fail by stopping, and may restart.
  They all have persistent storage.
- Messages can take arbitrarily long to be delivered, can be duplicated, and can
  be lost, but they're not corrupted.

*Choosing a value:*
- A proposal contains an unique number (generated by the proposal) and the value

|---------+----------------------------------------+---------------------------------------|
|         | Proposer                               | Acceptor                              |
|---------+----------------------------------------+---------------------------------------|
| Phase 1 | Choose n                               | if a prepare(n) msg is received,      |
|         | send prepare(n)                        | and n is greater than any prepare msg |
|         |                                        | received thus far, repond with the    |
|         |                                        | highest numbered accept msg known     |
|---------+----------------------------------------+---------------------------------------|
| Phase 2 | v <- value of the highest numbered msg | if an accept(n,v) is received,        |
|         | send accept(n,v)                       | accept proposal if no                 |
|         |                                        | prepare(n'), n'>n is responded        |
|---------+----------------------------------------+---------------------------------------|

- Because of message loss, a value could be chosen with no learner ever finding
  out. In that case, learners will find out what value is chosen only when a new
  proposal is chosen.

- To guarantee *progress*, *a distinguished proposer* must be selected as the
  only one to try issuing proposals. If it can communicate successfully with a
  majority of acceptors, and if it uses a proposal with number greater than any
  already used, then it will succeed in issuing a proposal that will be
  accepted.

- There's a *leader* which acts as both the *distinguished {proposer,learner}*.

- It assumes that there's an algorithm that used to elect a single leader.

*Implementing a State Machine:*

- When a new leader is selected, it tries to fill its gaps first by running the
  phase 1 of the algorithm for those instances. 

  It'll either figure out the value, or realize that they're unused and declare
  them as "no-op" in the phase 2 of those instances.
  
  - _Example:_ Assume that it knows the values 1-134, 138 & 139. Then it'll run
    phase 1 for 135-137 and of all instances greater than 139. As long as enough
    of the system (acceptors and the communication network) is working properly,
    this search will end in some point.

    Suppose that we're able to determine the value to be proposed in instances
    135 & 140. The leader then executes phase 2 for those instances, and phase 2
    with "no-op" for 136 & 137.

    Now, it's free to pick any value for the instances > 140 since it has
    already executed the phase 1 for those instances.

- If *multiple servers think that they're the leaders*, they can all propose
  values in the same instance of the consensus algorithm, which could prevent
  any value from being chosen. However, *safety is preserved*. And election of a
  *single leader is needed only to ensure progress*.

* Unread
** Paxos Made Live - An Engineering Perspective

[[https://www.cs.utexas.edu/users/lorenzo/corsi/cs380d/papers/paper2-1.pdf][link]]

(Authors are from Google Research)

*Abstract*: How to build fault-tolerant data base using the Paxos algorithm.
They showed that, even though the existing literature, it's still a non-trivial
task. They list the problems they encountered and their solutions.

*** Introduction
    
- A page of Paxos code resulted in *several thousands of C++ code*, due to the
  implementation of many features and optimizations.

- Proving a page of pseudo-code correct *doesn't scale* to thousands of production
  code. They claim that to gain confidence in the "correctness" of a real
  system, different methods had to be used.

- Algorithms tolerate a *limited set of faults*.
  
- A real system is *rarely specified precisely*. Specification may change during
  implementation, fail due to a misunderstanding during its specification phase.
  
*** Architecture

- *Fault-tolerant replicated log*: Based on the Paxos algorithm. Each replica
  contains a local copy. Paxos is run repeatedly to ensure that each replica has
  the same log.

- *Fault-tolerant replicated database*: Consists of a local /snapshot/ and a
  /replay-log/ of database operations. New database operations are submitted to
  the replicated log.

- Chubby clients communicate with a *single Chubby replica* through a
  Chubby-specific protocol.
  
- The log is general enough to be used in other systems.
  
*** Basic Paxos
Algorithm:

1. Elect a replica to be the /coordinator/
2. The coordinator selects a value and broadcasts it to all replicas, called the
   /accept/ message.
3. Other replicas either /acknowledge/ this message or /reject/ it.
4. Once a majority of the replicas acknowledge the coordinator, consensus has
   been reached, and the coordinator broadcasts a /commit/ message.

Details:
1. Multiple replicas may decide to become coordinators.

2. It assigns an *ordering* to successive coordinators. 
   
   For example, if there are n replicas, each one is assigned an id $i_r$ between
   0 and (n-1). When they want to be the new coordinator, they pick a value $s$
   that is greater than anything *it has seen so far* and $s \mod i_r = 0$.

3. After generating a new id, it's broadcast to all replicas in a /propose/
   message. If the replicas agree (that they haven't seen a coordinator with a
   higher number), they respond with a /promise/ message.
  
4. *Promise messages also contain the latest value they heard of* with the id of
   the coordinator (if any). The new coordinator picks the value of the most
   recent one. If none of the promise messages contain a value, it's free to pick
   any value.

5. If a consensus was achieved by the previous coordinator, the new one *obeys* it
   (since it waits for a promise message from the majority of the nodes).
   
#+BEGIN_SRC 
Client   Proposer      Acceptor
   |         |          |  |  |
   X-------->|          |  |  |   Request
   |         |          |  |  |
   |         X--------->|->|->|   Propose(1)
   |         |<---------X--X--X   Promise(1,{Va,Vb,Vc})
   |         |          |  |  |
   |         X--------->|->|->|   Accept!(1,Vn)
   |         |<---------X--X--X   Acknowledgment(1,Vn)
   |         |          |  |  |
   |<-------------------X--X--X   Commit
   |         |          |  |  |
#+END_SRC

*** Multi-Paxos

After a crash, the replica *replays* the persistent log to catch up.

Thus, the algorithm requires a sequence of 5 *writes to disk* (propose, promise,
accept, acknowledgment, commit) on its critical path and they must be *flushed*
to disk. This latency might *dominate* if the nodes are in a close network.

A well-known optimization: *Omit propose messages* if coordinator stays the
same. In order to do that, coordinators are selected for *long periods of time*,
and they're called /master/.

Using this requires *only one write* to disk per Paxos instance:
- Master writes to disk immediately after sending an accept message
- Others write to disk prior to sending their acknowledge message

*** Challenges

Disk corruption:

** Verifying Fault-Tolerant Erlang Programs

ftp://ftp.inrialpes.fr/pub/vasy/publications/others/Benac-Fredlund-Derrick-05.pdf

** Analysing Fault Tolerance for Erlang Applications

https://uu.diva-portal.org/smash/get/diva2:213697/FULLTEXT01.pdf

** Verifying fault-tolerant Erlang programs

https://www.researchgate.net/publication/221211408_Verifying_fault-tolerant_Erlang_programs

** SMT and POR beat Counter Abstraction: Parametrized Model Checking of Threshold-Based Distributed Algorithms

http://forsyte.at/download/konnov-cav15.pdf

** What You Always Wanted to Know About Model Checking of Fault-Tolerant Distributed Algorithms

https://www.researchgate.net/publication/304480515_What_You_Always_Wanted_to_Know_About_Model_Checking_of_Fault-Tolerant_Distributed_Algorithms

** Experience with Rules-Based Programming for Distributed, Concurrent, Fault-Tolerant Code

https://www.usenix.org/system/files/conference/atc15/atc15-paper-stutsman.pdf

** What's Decidable About Arrays?

[[http://www-step.stanford.edu/papers/avmcai06.pdf][link]]

** Counting Constraints in Flat Array Fragments

[[https://link.springer.com/chapter/10.1007/978-3-319-40229-1_6][link]]


