#+TITLE: Reading List
#+OPTIONS: toc:t html-postamble:nil tex:t

# bigblow
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/bigblow.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/hideshow.css"/>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-1.11.0.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.localscroll-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.zclip.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/bigblow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/hideshow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>


* Read
** Building a Distributed Fault-Tolerant Key-Value Store

[[http://blog.fourthbit.com/2015/04/12/building-a-distributed-fault-tolerant-key-value-store][link]]

- One coordinator per data-center, elected using a paxos-like consensus protocol.

- *Hinted-handoff*: Cassandra has replication of data. When you try to write
  data but a replica is down, the coordinator keeps an extra local copy per
  failed replica, until it's back up.

- *Committing a write*:
  1. Persistent memory (disk) in a commit log
  2. In-memory memtable

- Since any node can act as a request coordinator, it is necessary to keep all
  nodes informed at all times of the other nodes in the ring. Such problem is
  addressed by the so-called *membership protocol*.

- *SWIM* (Scalable Weakly-consistent Infection-style Process Group Membership)

- *Eventual consistency*: If no new updates are made, eventually all accesses to
  that item will return the last updated value. Using *read repair*, the
  correction is done when a read finds an inconsistent

- *Always writable*: Write is like appending data with a timestamp into a file.
  The burden of data consistency is brought to read time.

- *Stabilization*: When a node fails, =key->node= mapping breaks for some keys.
  As it fails, there will be n-1 nodes for key distribution, meaning that many
  keys will suddenly have a different set of nodes assigned.

  A possible implementation would be iterating over all keys in a node,
  detecting if the key still belongs to the node and if that's not the case,
  delete it from local storage after sending it to all proper replicas.

** Eventual Consistency

[[https://distributedthoughts.wordpress.com/2013/09/08/eventual-consistency/][link]]

#+BEGIN_SRC
1. write(7, "A")
2. write(7, "B")
3. read(7)
4. read(7)
#+END_SRC

in an eventual consistent db, all four results are legal (both lines 3 & 4 can
return "A" or "B"). For example, line 3 can return "B" and line 4 can return
"A", it can /disappear/.

*Why?* Read/write operations can be implemented at a lower latency than strongly
consistent databases.

Requiring low latency means that if a machine receives a read operation, it
should answer that read operation *immediately*. In other words, if it receives a
read operation it *canâ€™t contact another server* before responding.

** Understanding Paxos

[[https://distributedthoughts.wordpress.com/2013/09/22/understanding-paxos-part-1/][link]]

Possible *API* formalization:
  * =add(item)= : tries to add item to the log, and returns the index (or -1)
  * =get(index)= : returns the item at the given index (or =null=)
  * an event triggered each time an item is added, providing the item and its
    index as parameters

When you have only 2 nodes, due to the [[http://en.wikipedia.org/wiki/Split-brain_(computing)][split brain]] problem, you can't store data
in a fault resilient manner. *Paxos ensures that* no matter what if data is
written, it will eventually propagato to all the nodes and there won't be any
nodes that think some index contains different values (conflicting log records).

*Paxos assumes that*:
  * the content of messages aren't changed after they're sent
  * nodes run the algorithm as instructed (no bugs, hackers, etc.)
  * nodes have persistent storage

*Paxos ensures "safety" even if*:
  * messages are dropped or duplicated
  * some (or all) of the nodes crash or restart

#+BEGIN_QUOTE
/Paxos always guarantees "safety", while ensuring "liveness" if a majority of the
nodes are communicating with each other./
#+END_QUOTE

Paxos has *three main blocks*:
  1. Leader election protocol - to decide which node is running the show
  2. Consensus on a single log entry (called *Synod*) - preserves "safety"
  3. A protocol for managing the entire log - what entries should be added

A *bird's eye view* of the algorithm:
  * continuously (every X seconds) run the leader election algorithm, so all
    nodes know who's in charge (Paxos doesn't specify which algorithm to use)
  * when the leader receives a request to add a log item =ITEM=, it selects the
    next empty log index =INDEX=, and initiates a consensus on adding
    =(INDEX,ITEM)= to the log.
  * when a consensus is initiated to add =(INDEX,ITEM)=, all nodes participate
    in it, eventually agreeing that item =ITEM= was added to the log at index
    =INDEX=.

If *more than one leader* is elected, and two different leaders initiate a
consensus on =(INDEX,ITEM1)= and =(INDEX,ITEM2)=, Synod algorithm will choose
only one of them.

When the leader is elected, it goes over the log and sees if there are any
*holes* in it. Assume that index 17 is empty, the leader would then initiate the
Synod algorithm with the value =(17,EMPTY)=, and the algorithm will reach to a
consensus on the value =(17,EMPTY)= or =(17,VALUE)= for some previously proposed
value.

Once there are *no more holes*, the leader can start processing writes: for each
request to add an item to the log, the leader initiates the consensus algorithm
with =(INDEX,ITEM)= where =INDEX= is the next empty slot.

*To sum up*, Paxos:
1. is a highly resilient algorithm providing an abstraction of a
   distributed log.
2. assumes some mechanism of electing a leader.
3. ensures that data log is consistent, even if there are none or multiple
   leaders.
4. will be able to perform writes, if more than half of the nodes can
   communicate.

** Paxos Made Simple

[[http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf][link]]

*Problem:* A collection of networked processes propose values, we have to pick
one.

*Safety requirements:*
- a single one among the proposed values is chosen
- if no value is proposed, no value should be chosen
- if a value has been chosen, then processes should be able to learn the chosen
  value

*Processes:*
- 3 classes of agents: /proposers/, /acceptors/ and /learners/.
- Agents operate at different speeds, may fail by stopping, and may restart.
  They all have persistent storage.
- Messages can take arbitrarily long to be delivered, can be duplicated, and can
  be lost, but they're not corrupted.

*Choosing a value:*
- A proposal contains an unique number (generated by the proposal) and the value

|---------+----------------------------------------+---------------------------------------|
|         | Proposer                               | Acceptor                              |
|---------+----------------------------------------+---------------------------------------|
| Phase 1 | Choose n                               | if a prepare(n) msg is received,      |
|         | send prepare(n)                        | and n is greater than any prepare msg |
|         |                                        | received thus far, repond with the    |
|         |                                        | highest numbered accept msg known     |
|---------+----------------------------------------+---------------------------------------|
| Phase 2 | v <- value of the highest numbered msg | if an accept(n,v) is received,        |
|         | send accept(n,v)                       | accept proposal if no                 |
|         |                                        | prepare(n'), n'>n is responded        |
|---------+----------------------------------------+---------------------------------------|

- Because of message loss, a value could be chosen with no learner ever finding
  out. In that case, learners will find out what value is chosen only when a new
  proposal is chosen.

- To guarantee *progress*, *a distinguished proposer* must be selected as the
  only one to try issuing proposals. If it can communicate successfully with a
  majority of acceptors, and if it uses a proposal with number greater than any
  already used, then it will succeed in issuing a proposal that will be
  accepted.

- There's a *leader* which acts as both the *distinguished {proposer,learner}*.

- It assumes that there's an algorithm that used to elect a single leader.

*Implementing a State Machine:*

- When a new leader is selected, it tries to fill its gaps first by running the
  phase 1 of the algorithm for those instances.

  It'll either figure out the value, or realize that they're unused and declare
  them as "no-op" in the phase 2 of those instances.

  - _Example:_ Assume that it knows the values 1-134, 138 & 139. Then it'll run
    phase 1 for 135-137 and of all instances greater than 139. As long as enough
    of the system (acceptors and the communication network) is working properly,
    this search will end in some point.

    Suppose that we're able to determine the value to be proposed in instances
    135 & 140. The leader then executes phase 2 for those instances, and phase 2
    with "no-op" for 136 & 137.

    Now, it's free to pick any value for the instances > 140 since it has
    already executed the phase 1 for those instances.

- If *multiple servers think that they're the leaders*, they can all propose
  values in the same instance of the consensus algorithm, which could prevent
  any value from being chosen. However, *safety is preserved*. And election of a
  *single leader is needed only to ensure progress*.

** Paxos Made Live - An Engineering Perspective

[[https://www.cs.utexas.edu/users/lorenzo/corsi/cs380d/papers/paper2-1.pdf][link]]

(Authors are from Google Research)

*Abstract*: How to build fault-tolerant data base using the Paxos algorithm.
They showed that, even though the existing literature, it's still a non-trivial
task. They list the problems they encountered and their solutions.

*** Introduction

- A page of Paxos code resulted in *several thousands of C++ code*, due to the
  implementation of many features and optimizations.

- Proving a page of pseudo-code correct *doesn't scale* to thousands of production
  code. They claim that to gain confidence in the "correctness" of a real
  system, different methods had to be used.

- Algorithms tolerate a *limited set of faults*.

- A real system is *rarely specified precisely*. Specification may change during
  implementation, fail due to a misunderstanding during its specification phase.

*** Architecture

- *Fault-tolerant replicated log*: Based on the Paxos algorithm. Each replica
  contains a local copy. Paxos is run repeatedly to ensure that each replica has
  the same log.

- *Fault-tolerant replicated database*: Consists of a local /snapshot/ and a
  /replay-log/ of database operations. New database operations are submitted to
  the replicated log.

- Chubby clients communicate with a *single Chubby replica* through a
  Chubby-specific protocol.

- The log is general enough to be used in other systems.

*** Basic Paxos
Algorithm:

1. Elect a replica to be the /coordinator/
2. The coordinator selects a value and broadcasts it to all replicas, called the
   /accept/ message.
3. Other replicas either /acknowledge/ this message or /reject/ it.
4. Once a majority of the replicas acknowledge the coordinator, consensus has
   been reached, and the coordinator broadcasts a /commit/ message.

Details:
1. Multiple replicas may decide to become coordinators.

2. It assigns an *ordering* to successive coordinators.

   For example, if there are n replicas, each one is assigned an id $i_r$ between
   0 and (n-1). When they want to be the new coordinator, they pick a value $s$
   that is greater than anything *it has seen so far* and $s \mod i_r = 0$.

3. After generating a new id, it's broadcast to all replicas in a /propose/
   message. If the replicas agree (that they haven't seen a coordinator with a
   higher number), they respond with a /promise/ message.

4. *Promise messages also contain the latest value they heard of* with the id of
   the coordinator (if any). The new coordinator picks the value of the most
   recent one. If none of the promise messages contain a value, it's free to pick
   any value.

5. If a consensus was achieved by the previous coordinator, the new one *obeys* it
   (since it waits for a promise message from the majority of the nodes).

*** Multi-Paxos

After a crash, the replica *replays* the persistent log to catch up.

Thus, the algorithm requires a sequence of 5 *writes to disk* (propose, promise,
accept, acknowledgment, commit) on its critical path and they must be *flushed*
to disk. This latency might *dominate* if the nodes are in a close network.

A well-known optimization: *Omit propose messages* if coordinator stays the
same. In order to do that, coordinators are selected for *long periods of time*,
and they're called /master/.

Using this requires *only one write* to disk per Paxos instance:
- Master writes to disk immediately after sending an accept message
- Others write to disk prior to sending their acknowledge message

#+BEGIN_SRC
%%% MULTI PAXOS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  | --- First Request ---
   X-------->|          |  |  |       |  |  Request
   |         |          |  |  |       |  |
   |         X--------->|->|->|       |  |  Prepare(N)
   |         |<---------X--X--X       |  |  Promise(N,I,{Va,Vb,Vc})
   |         |          |  |  |       |  |
   |         X--------->|->|->|       |  |  Accept!(N,I,Vm)
   |         |<---------X--X--X------>|->|  Accepted(N,I,Vm)
   |         |          |  |  |       |  |
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |
#+END_SRC

*** Challenges

**** Disk corruption
- Either the file content may change or file may become inaccessible.

- In the case of corruption, use check-sum

- Disk rebuild is done by participating in Paxos as a non-voting member; it
  doesn't respond to promise or acknowledgment messages

- Since we can allow disk corruption, it may be acceptable not to flush writes
  to disk immediately

**** Master leases
- In basic Paxos, clients cannot read from master since other replicas can elect
  another master and modify the data structure without notifying the old master.

- The workaround is to implement /master leases/. As long as the master has the
  lease, it's guaranteed that other replicas cannot successfully submit values
  to Paxos. Thus, master with the lease has up-to-date information in its local
  data.

- The master periodically submits a dummy "heartbeat" value to Paxos to refresh
  its lease.

- Problem: When a master temporarily disconnects, Paxos will elect a new master.
  The new master will maintain a fixed sequence number across instances of
  Paxos. In the mean time, when the disconnected old master tries to run the
  Paxos algorithm, if it *manages to connect with another replica*, it may
  *increase its sequence number*.

  - In their implementation, the master periodically boosts its sequence number
    by running a full round of Paxos algorithm. Boosting with the right
    frequency generally avoids this case.

**** Epoch numbers

- Chubby requires an incoming request to be aborted since the mastership can be
  lost and/or re-acquired during the handling of the request.

- So they needed to detect master turnover and abort operations if necessary

- They've solved this problem by using *epoch numbers*: Two requests for the
  epoch number at the master replica receive the same value iff that replica was
  master continuously for the time interval between the two requests.

- The epoch number is stored as an entry in the database, and all database
  operations are made conditional on the value of the epoch number.

**** Group membership

- Systems must be able to handle changes in the set of replicas, which is
  referred as the *group membership*.
- This is trivial for basic Paxos, but the details are non-trivial for the
  Multi-Paxos case with disk corruptions, etc.
- They had to fill these details by themselves, which they don't mention in the
  paper.

**** Snapshots

- The log can grow indefinitely which requires *unbounded disk space* and
  *unbounded recovery time* after a failure. One way is to persist the changes
  in the log directly, which is called a /snapshot/.

- Since Paxos only worries about the replicated log, the application itself
  should figure out how to create those snapshots.

- The fault-tolerant database informs the framework that a snapshot was taken;
  and it's is free to take a snapshot at any point.

- When the Paxos framework is informed about a snapshot, it will truncate its
  log by deleting log entries that precede the snapshot.

- During recovery replica will install the latest snapshot and then replay the
  truncated log to rebuild its state.

- Snapshots are not synchronized across replicas; each replica independently
  decides when to create a snapshot.

- *Problems:* Persistent state of the replica now contains the log, and
  snapshots have to be maintained consistently.

- Snapshots and logs need to be mutually consistent ==> Snapshot handle

- Taking a snapshot takes time, so it's split into 3 phases. The replica:
  1. Requests a *snapshot handle*. It contains the Paxos instance number
     corresponding to the snapshot and the group membership at that point.
  2. *Takes a snapshot*. It may block the system while taking the snapshot, or
     (more likely) spawn a thread that takes a snapshot while the replica
     continues to participate in Paxos. It's important to keep the local data
     consistent while participating in Paxos.
  3. When the snapshot has been taken, the client application *informs* the
     framework about the snapshot and passes the corresponding snapshot handle.
     The framework then truncates the log appropriately.

- Taking a snapshot may *fail*, so the framework truncates the log when it
  receives a snapshot handle.

- While in *catch-up*, a replica will attempt to obtain missing log records. If
  it cannot obtain them (because no replica has old-enough log entries), the
  replica will be told to *obtain a snapshot* from another replica.

  Note that a leading replica may even *create a new snapshot* while a lagging
  replica is installing an *older snapshot*. In this case, the lagging replica
  may not be able to obtain any outstanding log records because the snapshot
  provider may have moved ahead. The lagging replica will need to obtain a more
  recent snapshot.

  Furthermore, the *leading replica may fail* after sending its snapshot. The
  catch-up mechanism must be able to recover from such problems by having the
  lagging replica contact another leading replica.

- A mechanism to *locate recent snapshots* is needed.

**** Database transactions

- *Chubby* stores key-value pairs, and supports =insert=, =delete=, =lookup=, =cas=
  (atomic compare & swap), and iteration over all entries.

- The =cas= operation needed to be atomic with respect to other database
  operations issued by a different replica. It was achieved by *submitting all
  =cas=-related data as a single value* to Paxos. This mechanism is extended to
  provide transaction-style support without having to implement true database
  transactions.

- A primitive called =MultiOp= is used to implement all database functionality
  (except iteration). It consists of 3 components:

  1. A list of tests called *guard*. Each test in =guard= checks a single db
     entry. If all tests are true, =MultiOp= executes =t_op=, otherwise it
     executes =f_op=.

  2. A list of db operations called =t_op=. Each operation is either =insert=,
     =delete=, or =lookup=, and applies to a single db entry.

  3. A list of db operations called =f_op=, which is like =t_op=.

*** Software Engineering

**** Expressing the algorithm effectively

- They've coded the *core algorithm* as two state machines. They designed a
  simple statemachine specification language and built a compiler to translate
  such specifications into C++.

- Choosing a *specification language* makes it *easier to reason about and
  modify* our state machines than an explicitly coded implementation that is
  intermingled with the rest of the system.

  - They share their experience where they moved from 3 states to 2 states in
    the group membership algorithm. It took them about *one hour to make this
    change* and *three days to modify their tests* accordingly.

**** Runtime consistency checking

- The master periodically submits a *checksum request* to the database log. On
  receipt of this request, each replica computes a checksum of its local
  database (they used a shadow data structure to handle db operations
  concurrently).
- After the master completes a checksum computation, it sends its checksum to
  all replicas which *compare* the masterâ€™s checksum with their computed checksum.
- They had 3 db inconsistency incidents:
  - First was operator error
  - Second was probably due to hardware memory corruption
  - Third was probably due to illegal memory access from errant code.

**** Testing

- Tests have 2 modes: *safety* and *liveness*.

- Tests start in safety mode and *inject random failures* into the system. After
  running for a predetermined period of time, they stop injecting failures and
  give the system time to fully recover. Then they switch the test to liveness
  mode. The purpose for the liveness test is to verify that the system does not
  deadlock after a sequence of failures.

- Fault-tolerant systems try to *mask problems*. Thus, they can mask bugs or
  configuration problems while *lowering their own fault-tolerance*.
  - They tell the story of a typo in the configuration that result in a scenario
    where one of the five replicas always run in catch-up mode (which appears
    correct). This made the system tolerate only one faulty component (instead
    of two).

**** Concurrency

They claim that they set the "right goals" for repeatability of executions by
*constraining concurrency*. Unfortunately, as the product needs grew they were
unable to adhere to these goals, and added more concurrency support (to take
snapshots, compute checksums and process iterators while concurrently serving
database requests).

*** Unexpected Failures

These are the failures they've faced when they tried to move from the 3DB
version of Chubby to Paxos version during the course of *100 machine years*:

- First release shipped with ten times the number of worker threads as the
  original Chubby system. Under load, the worker threads ended up *starving*
  some other key threads and caused the system to time out frequently. This
  resulted in rapid master fail over, which resulted in a *cycle* of new master
  election, and master fail over. They've lost 15 hours of data.

- They had to address the changes in the difference in semantics. For example,
  if Chubby submitted an operation to the db, and the db lost its master status,
  Chubby expected the operation to fail. However, in the new system a replica
  could be re-installed as master, and operation could succeed. The fix took a
  substantial work and required the use of *epoch numbers* and =MultiOp=.

- They also had issues regarding to the OS: In Linux 2.4 kernel, when they try
  to flush a small file to disk, the call can hang for a long time if there are
  a lot of buffered writes to other files. The workaround was to write all large
  files in chunk.

- In total:
  - 3 failures occurred during upgrade (or rollback).
  - 2 were from bugs
  - 2 were due to operator errors. While they are very competent, they are
    usually not part of the development team that built the system, and
    therefore not familiar with its intricate details.
  - 1 was due to memory corruption

*** Measurements

- Tests ran two copies of Chubby on the same set of 5 servers (typical
  Pentium-class machines).
- Speed-up is between 1.2x to 4.4x

*** Open Problems

- There are *significant gaps* between the description of the Paxos algorithm
  and the needs of a real-world system. In order to build a real-world system,
  an expert needs to *use numerous ideas scattered in the literature* and make
  several relatively small protocol extensions. The cumulative effort will be
  substantial and the final system will be based on an *unproven protocol*.

- The fault-tolerance computing community *has not developed the tools* to make
  it easy to implement their algorithms.

- The fault-tolerance computing community *has not paid enough attention to
  testing*, a key ingredient for building fault-tolerant systems.

* Unread
** Verdi: a framework for implementing and formally verifying distributed systems

[[http://verdi.uwplse.org/verdi.pdf][link]]

*** Introduction

- Few practical distributed system implementations have been formally verified.
  For performance reasons, real-world implementations often diverge in important
  ways from their high-level descriptions. So, goal is to *verify working code*.
  *Coq* is used to both write and verify the system.

- Distributed systems run in a diverse range of environments. Thus, it must support
  verifying applications against *different fault models*. It's specified as
  *network semantics*.

- Verdi aims to help the programmer separately prove correctness of application
  level behavior and correctness of fault tolerance mechanisms, and to allow
  these proofs to be easily *composed*.

  - They introduce *verified system transformers*, which is a function whose input
    is an implementation of a system and whose output is a *new system
    implementation* that makes different assumptions about its environment.

  - For example, a Verdi programmer can first build and verify a system assuming a
    reliable network, and then apply a transformer to obtain another version of
    their system that correctly and provably tolerates faults in an unreliable
    network.

- They implemented and verified
  - a key-value store,
  - a primary-backup replication transformer
  - first formally verified proof of linearizability for the Raft consensus protocol

- *Recent bug*: In April 2011 a malfunction of failure recovery in EC2 caused a
  major outage and brought down several web sites, including Foursquare, Reddit,
  Quora, and PBS.

**** TODO what is a replication transformer ?

*** Overview

- Programmer provides the specification, implementation (naming of nodes, IO for
  each node + msg that it responds to, state of each node, msg handling code
  that each node runs), and verification of this implementation that runs on a
  *idealized reliable model*.

- Then a target network semantics that reflects environment's fault model is
  selected, and a VST is applied to the implementation to transform it into one
  that is correct in that fault model. This transformation also produces updated
  versions of the specification and proof.

- The following are the steps taken to implement a lock service (one server):

**** Specification

- Correct behavior is specified in terms of *traces*

\begin{align*}
mutex(\tau) := & \tau = \tau_1 \mathtt{++}
\langle n_1, \mathtt{Grant} \rangle \mathtt{++}
\tau_2 \mathtt{++}
\langle n_2, \mathtt{Grant} \rangle
\mathtt{++} \tau_3 \\
& \rightarrow \langle n_1, \mathtt{Unlock} \rangle \in \tau_2
\end{align*}

#+BEGIN_SRC ocaml
(* 1 - node identifiers *)
Name := Server | Agent(int)

(* 2 - API, also known as external IO *)
Inp := Lock | Unlock
Out := Grant
(* 2 - network messages *)
Msg := LockMsg | UnlockMsg | GrantMsg

(* 3 - state *)
State (n: Name) :=
  match n with
  | Server => list Name (* head = agent holding lock *)
                        (* tail = agents waiting for lock *)
  | Agent n => bool     (* true iff this agent holds lock *)

InitState (n: Name) : State n :=
  match n with
  | Server => []
  | Agent n => false

(* 4 - handler for external input *)
HandleInp (n: Name) (s: State n) (inp: Inp) :=
  match n with
  | Server => nop                 (* server performs no external IO *)
  | Agent n =>
    match inp with
    | Lock =>                     (* client requests lock *)
        send (Server, LockMsg)    (* forward to Server *)
    | Unlock =>                   (* client requests unlock *)
      if s == true then (         (* if lock held *)
        s := false;;              (* update state *)
        send (Server, UnlockMsg)) (* tell Server lock freed *)

(* 4 - handler for network messages *)
HandleMsg (n: Name) (s: State n) (src: Name) (msg: Msg) :=
  match n with
  | Server =>
    match msg with
    | LockMsg =>
        (* if lock not held, immediately grant *)
        if s == [] then send (src, GrantMsg);;
        (* add requestor to end of queue *)
        s := s ++ [src]
    | UnlockMsg =>
      (* head of queue no longer holds lock *)
      s := tail s;;
      (* grant lock to next waiting agent, if any *)
      if s != [] then send (head s, GrantMsg)
    | _ => nop (* never happens *)
  | Agent n =>
    match msg with
    | GrantMsg =>               (* lock acquired *)
      s := true;;               (* update state *)
      output Grant              (* notify listeners *)
     | _ => nop                 (* never happens *)
#+END_SRC


** Ivy: Safety Verification by Interactive Generalization

[[https://www.cs.tau.ac.il/~odedp/ivy.pdf][link]]

** Verifying Fault-Tolerant Erlang Programs

[[ftp://ftp.inrialpes.fr/pub/vasy/publications/others/Benac-Fredlund-Derrick-05.pdf][link]]

** Analyzing Fault Tolerance for Erlang Applications

[[https://uu.diva-portal.org/smash/get/diva2:213697/FULLTEXT01.pdf][link]]

** Verifying fault-tolerant Erlang programs

[[https://www.researchgate.net/publication/221211408_Verifying_fault-tolerant_Erlang_programs][link]]

** SMT and POR beat Counter Abstraction: Parametrized Model Checking of Threshold-Based Distributed Algorithms

[[http://forsyte.at/download/konnov-cav15.pdf][link]]

** What You Always Wanted to Know About Model Checking of Fault-Tolerant Distributed Algorithms

[[https://www.researchgate.net/publication/304480515_What_You_Always_Wanted_to_Know_About_Model_Checking_of_Fault-Tolerant_Distributed_Algorithms][link]]

** Experience with Rules-Based Programming for Distributed, Concurrent, Fault-Tolerant Code

[[https://www.usenix.org/system/files/conference/atc15/atc15-paper-stutsman.pdf][link]]

** What's Decidable About Arrays?

[[http://www-step.stanford.edu/papers/avmcai06.pdf][link]]

** Counting Constraints in Flat Array Fragments

[[https://link.springer.com/chapter/10.1007/978-3-319-40229-1_6][link]]


