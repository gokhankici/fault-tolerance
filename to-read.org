#+TITLE: Reading List
#+OPTIONS: toc:t html-postamble:nil tex:t

# bigblow
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/bigblow.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="http://www.pirilampo.org/styles/bigblow/css/hideshow.css"/>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-1.11.0.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery-ui-1.10.2.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.localscroll-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.scrollTo-1.4.3.1-min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/jquery.zclip.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/bigblow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/bigblow/js/hideshow.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="http://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>


* Read
** Building a Distributed Fault-Tolerant Key-Value Store

[[http://blog.fourthbit.com/2015/04/12/building-a-distributed-fault-tolerant-key-value-store][link]]

- One coordinator per data-center, elected using a paxos-like consensus protocol.

- *Hinted-handoff*: Cassandra has replication of data. When you try to write
  data but a replica is down, the coordinator keeps an extra local copy per
  failed replica, until it's back up.

- *Committing a write*:
  1. Persistent memory (disk) in a commit log
  2. In-memory memtable

- Since any node can act as a request coordinator, it is necessary to keep all
  nodes informed at all times of the other nodes in the ring. Such problem is
  addressed by the so-called *membership protocol*.

- *SWIM* (Scalable Weakly-consistent Infection-style Process Group Membership)

- *Eventual consistency*: If no new updates are made, eventually all accesses to
  that item will return the last updated value. Using *read repair*, the
  correction is done when a read finds an inconsistent

- *Always writable*: Write is like appending data with a timestamp into a file.
  The burden of data consistency is brought to read time.

- *Stabilization*: When a node fails, =key->node= mapping breaks for some keys.
  As it fails, there will be n-1 nodes for key distribution, meaning that many
  keys will suddenly have a different set of nodes assigned.

  A possible implementation would be iterating over all keys in a node,
  detecting if the key still belongs to the node and if that's not the case,
  delete it from local storage after sending it to all proper replicas.

** Eventual Consistency

[[https://distributedthoughts.wordpress.com/2013/09/08/eventual-consistency/][link]]

#+BEGIN_SRC
1. write(7, "A")
2. write(7, "B")
3. read(7)
4. read(7)
#+END_SRC

in an eventual consistent db, all four results are legal (both lines 3 & 4 can
return "A" or "B"). For example, line 3 can return "B" and line 4 can return
"A", it can /disappear/.

*Why?* Read/write operations can be implemented at a lower latency than strongly
consistent databases.

Requiring low latency means that if a machine receives a read operation, it
should answer that read operation *immediately*. In other words, if it receives a
read operation it *canâ€™t contact another server* before responding.

** Understanding Paxos

[[https://distributedthoughts.wordpress.com/2013/09/22/understanding-paxos-part-1/][link]]

Possible *API* formalization:
  * =add(item)= : tries to add item to the log, and returns the index (or -1)
  * =get(index)= : returns the item at the given index (or =null=)
  * an event triggered each time an item is added, providing the item and its
    index as parameters

When you have only 2 nodes, due to the [[http://en.wikipedia.org/wiki/Split-brain_(computing)][split brain]] problem, you can't store data
in a fault resilient manner. *Paxos ensures that* no matter what if data is
written, it will eventually propagato to all the nodes and there won't be any
nodes that think some index contains different values (conflicting log records).

*Paxos assumes that*:
  * the content of messages aren't changed after they're sent
  * nodes run the algorithm as instructed (no bugs, hackers, etc.)
  * nodes have persistent storage

*Paxos ensures "safety" even if*:
  * messages are dropped or duplicated
  * some (or all) of the nodes crash or restart

#+BEGIN_QUOTE
/Paxos always guarantees "safety", while ensuring "liveness" if a majority of the
nodes are communicating with each other./
#+END_QUOTE

Paxos has *three main blocks*:
  1. Leader election protocol - to decide which node is running the show
  2. Consensus on a single log entry (called *Synod*) - preserves "safety"
  3. A protocol for managing the entire log - what entries should be added

A *bird's eye view* of the algorithm:
  * continuously (every X seconds) run the leader election algorithm, so all
    nodes know who's in charge (Paxos doesn't specify which algorithm to use)
  * when the leader receives a request to add a log item =ITEM=, it selects the
    next empty log index =INDEX=, and initiates a consensus on adding
    =(INDEX,ITEM)= to the log.
  * when a consensus is initiated to add =(INDEX,ITEM)=, all nodes participate
    in it, eventually agreeing that item =ITEM= was added to the log at index
    =INDEX=.

If *more than one leader* is elected, and two different leaders initiate a
consensus on =(INDEX,ITEM1)= and =(INDEX,ITEM2)=, Synod algorithm will choose
only one of them.

When the leader is elected, it goes over the log and sees if there are any
*holes* in it. Assume that index 17 is empty, the leader would then initiate the
Synod algorithm with the value =(17,EMPTY)=, and the algorithm will reach to a
consensus on the value =(17,EMPTY)= or =(17,VALUE)= for some previously proposed
value.

Once there are *no more holes*, the leader can start processing writes: for each
request to add an item to the log, the leader initiates the consensus algorithm
with =(INDEX,ITEM)= where =INDEX= is the next empty slot.

*To sum up*, Paxos:
1. is a highly resilient algorithm providing an abstraction of a
   distributed log.
2. assumes some mechanism of electing a leader.
3. ensures that data log is consistent, even if there are none or multiple
   leaders.
4. will be able to perform writes, if more than half of the nodes can
   communicate.

** Paxos Made Simple

[[http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf][link]]

*Problem:* A collection of networked processes propose values, we have to pick
one.

*Safety requirements:*
- a single one among the proposed values is chosen
- if no value is proposed, no value should be chosen
- if a value has been chosen, then processes should be able to learn the chosen
  value

*Processes:*
- 3 classes of agents: /proposers/, /acceptors/ and /learners/.
- Agents operate at different speeds, may fail by stopping, and may restart.
  They all have persistent storage.
- Messages can take arbitrarily long to be delivered, can be duplicated, and can
  be lost, but they're not corrupted.

*Choosing a value:*
- A proposal contains an unique number (generated by the proposal) and the value

|---------+----------------------------------------+---------------------------------------|
|         | Proposer                               | Acceptor                              |
|---------+----------------------------------------+---------------------------------------|
| Phase 1 | Choose n                               | if a prepare(n) msg is received,      |
|         | send prepare(n)                        | and n is greater than any prepare msg |
|         |                                        | received thus far, repond with the    |
|         |                                        | highest numbered accept msg known     |
|---------+----------------------------------------+---------------------------------------|
| Phase 2 | v <- value of the highest numbered msg | if an accept(n,v) is received,        |
|         | send accept(n,v)                       | accept proposal if no                 |
|         |                                        | prepare(n'), n'>n is responded        |
|---------+----------------------------------------+---------------------------------------|

- Because of message loss, a value could be chosen with no learner ever finding
  out. In that case, learners will find out what value is chosen only when a new
  proposal is chosen.

- To guarantee *progress*, *a distinguished proposer* must be selected as the
  only one to try issuing proposals. If it can communicate successfully with a
  majority of acceptors, and if it uses a proposal with number greater than any
  already used, then it will succeed in issuing a proposal that will be
  accepted.

- There's a *leader* which acts as both the *distinguished {proposer,learner}*.

- It assumes that there's an algorithm that used to elect a single leader.

*Implementing a State Machine:*

- When a new leader is selected, it tries to fill its gaps first by running the
  phase 1 of the algorithm for those instances.

  It'll either figure out the value, or realize that they're unused and declare
  them as "no-op" in the phase 2 of those instances.

  - _Example:_ Assume that it knows the values 1-134, 138 & 139. Then it'll run
    phase 1 for 135-137 and of all instances greater than 139. As long as enough
    of the system (acceptors and the communication network) is working properly,
    this search will end in some point.

    Suppose that we're able to determine the value to be proposed in instances
    135 & 140. The leader then executes phase 2 for those instances, and phase 2
    with "no-op" for 136 & 137.

    Now, it's free to pick any value for the instances > 140 since it has
    already executed the phase 1 for those instances.

- If *multiple servers think that they're the leaders*, they can all propose
  values in the same instance of the consensus algorithm, which could prevent
  any value from being chosen. However, *safety is preserved*. And election of a
  *single leader is needed only to ensure progress*.

** Paxos Made Live - An Engineering Perspective

[[https://www.cs.utexas.edu/users/lorenzo/corsi/cs380d/papers/paper2-1.pdf][link]]

(Authors are from Google Research)

*Abstract*: How to build fault-tolerant data base using the Paxos algorithm.
They showed that, even though the existing literature, it's still a non-trivial
task. They list the problems they encountered and their solutions.

*** Introduction

- A page of Paxos code resulted in *several thousands of C++ code*, due to the
  implementation of many features and optimizations.

- Proving a page of pseudo-code correct *doesn't scale* to thousands of production
  code. They claim that to gain confidence in the "correctness" of a real
  system, different methods had to be used.

- Algorithms tolerate a *limited set of faults*.

- A real system is *rarely specified precisely*. Specification may change during
  implementation, fail due to a misunderstanding during its specification phase.

*** Architecture

- *Fault-tolerant replicated log*: Based on the Paxos algorithm. Each replica
  contains a local copy. Paxos is run repeatedly to ensure that each replica has
  the same log.

- *Fault-tolerant replicated database*: Consists of a local /snapshot/ and a
  /replay-log/ of database operations. New database operations are submitted to
  the replicated log.

- Chubby clients communicate with a *single Chubby replica* through a
  Chubby-specific protocol.

- The log is general enough to be used in other systems.

*** Basic Paxos
Algorithm:

1. Elect a replica to be the /coordinator/
2. The coordinator selects a value and broadcasts it to all replicas, called the
   /accept/ message.
3. Other replicas either /acknowledge/ this message or /reject/ it.
4. Once a majority of the replicas acknowledge the coordinator, consensus has
   been reached, and the coordinator broadcasts a /commit/ message.

Details:
1. Multiple replicas may decide to become coordinators.

2. It assigns an *ordering* to successive coordinators.

   For example, if there are n replicas, each one is assigned an id $i_r$ between
   0 and (n-1). When they want to be the new coordinator, they pick a value $s$
   that is greater than anything *it has seen so far* and $s \mod i_r = 0$.

3. After generating a new id, it's broadcast to all replicas in a /propose/
   message. If the replicas agree (that they haven't seen a coordinator with a
   higher number), they respond with a /promise/ message.

4. *Promise messages also contain the latest value they heard of* with the id of
   the coordinator (if any). The new coordinator picks the value of the most
   recent one. If none of the promise messages contain a value, it's free to pick
   any value.

5. If a consensus was achieved by the previous coordinator, the new one *obeys* it
   (since it waits for a promise message from the majority of the nodes).

*** Multi-Paxos

After a crash, the replica *replays* the persistent log to catch up.

Thus, the algorithm requires a sequence of 5 *writes to disk* (propose, promise,
accept, acknowledgment, commit) on its critical path and they must be *flushed*
to disk. This latency might *dominate* if the nodes are in a close network.

A well-known optimization: *Omit propose messages* if coordinator stays the
same. In order to do that, coordinators are selected for *long periods of time*,
and they're called /master/.

Using this requires *only one write* to disk per Paxos instance:
- Master writes to disk immediately after sending an accept message
- Others write to disk prior to sending their acknowledge message

#+BEGIN_SRC
%%% MULTI PAXOS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  | --- First Request ---
   X-------->|          |  |  |       |  |  Request
   |         |          |  |  |       |  |
   |         X--------->|->|->|       |  |  Prepare(N)
   |         |<---------X--X--X       |  |  Promise(N,I,{Va,Vb,Vc})
   |         |          |  |  |       |  |
   |         X--------->|->|->|       |  |  Accept!(N,I,Vm)
   |         |<---------X--X--X------>|->|  Accepted(N,I,Vm)
   |         |          |  |  |       |  |
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |
#+END_SRC

*** Challenges

**** Disk corruption
- Either the file content may change or file may become inaccessible.

- In the case of corruption, use check-sum

- Disk rebuild is done by participating in Paxos as a non-voting member; it
  doesn't respond to promise or acknowledgment messages

- Since we can allow disk corruption, it may be acceptable not to flush writes
  to disk immediately

**** Master leases
- In basic Paxos, clients cannot read from master since other replicas can elect
  another master and modify the data structure without notifying the old master.

- The workaround is to implement /master leases/. As long as the master has the
  lease, it's guaranteed that other replicas cannot successfully submit values
  to Paxos. Thus, master with the lease has up-to-date information in its local
  data.

- The master periodically submits a dummy "heartbeat" value to Paxos to refresh
  its lease.

- Problem: When a master temporarily disconnects, Paxos will elect a new master.
  The new master will maintain a fixed sequence number across instances of
  Paxos. In the mean time, when the disconnected old master tries to run the
  Paxos algorithm, if it *manages to connect with another replica*, it may
  *increase its sequence number*.

  - In their implementation, the master periodically boosts its sequence number
    by running a full round of Paxos algorithm. Boosting with the right
    frequency generally avoids this case.

**** Epoch numbers

- Chubby requires an incoming request to be aborted since the mastership can be
  lost and/or re-acquired during the handling of the request.

- So they needed to detect master turnover and abort operations if necessary

- They've solved this problem by using *epoch numbers*: Two requests for the
  epoch number at the master replica receive the same value iff that replica was
  master continuously for the time interval between the two requests.

- The epoch number is stored as an entry in the database, and all database
  operations are made conditional on the value of the epoch number.

**** Group membership

- Systems must be able to handle changes in the set of replicas, which is
  referred as the *group membership*.
- This is trivial for basic Paxos, but the details are non-trivial for the
  Multi-Paxos case with disk corruptions, etc.
- They had to fill these details by themselves, which they don't mention in the
  paper.

**** Snapshots

- The log can grow indefinitely which requires *unbounded disk space* and
  *unbounded recovery time* after a failure. One way is to persist the changes
  in the log directly, which is called a /snapshot/.

- Since Paxos only worries about the replicated log, the application itself
  should figure out how to create those snapshots.

- The fault-tolerant database informs the framework that a snapshot was taken;
  and it's is free to take a snapshot at any point.

- When the Paxos framework is informed about a snapshot, it will truncate its
  log by deleting log entries that precede the snapshot.

- During recovery replica will install the latest snapshot and then replay the
  truncated log to rebuild its state.

- Snapshots are not synchronized across replicas; each replica independently
  decides when to create a snapshot.

- *Problems:* Persistent state of the replica now contains the log, and
  snapshots have to be maintained consistently.

- Snapshots and logs need to be mutually consistent ==> Snapshot handle

- Taking a snapshot takes time, so it's split into 3 phases. The replica:
  1. Requests a *snapshot handle*. It contains the Paxos instance number
     corresponding to the snapshot and the group membership at that point.
  2. *Takes a snapshot*. It may block the system while taking the snapshot, or
     (more likely) spawn a thread that takes a snapshot while the replica
     continues to participate in Paxos. It's important to keep the local data
     consistent while participating in Paxos.
  3. When the snapshot has been taken, the client application *informs* the
     framework about the snapshot and passes the corresponding snapshot handle.
     The framework then truncates the log appropriately.

- Taking a snapshot may *fail*, so the framework truncates the log when it
  receives a snapshot handle.

- While in *catch-up*, a replica will attempt to obtain missing log records. If
  it cannot obtain them (because no replica has old-enough log entries), the
  replica will be told to *obtain a snapshot* from another replica.

  Note that a leading replica may even *create a new snapshot* while a lagging
  replica is installing an *older snapshot*. In this case, the lagging replica
  may not be able to obtain any outstanding log records because the snapshot
  provider may have moved ahead. The lagging replica will need to obtain a more
  recent snapshot.

  Furthermore, the *leading replica may fail* after sending its snapshot. The
  catch-up mechanism must be able to recover from such problems by having the
  lagging replica contact another leading replica.

- A mechanism to *locate recent snapshots* is needed.

**** Database transactions

- *Chubby* stores key-value pairs, and supports =insert=, =delete=, =lookup=, =cas=
  (atomic compare & swap), and iteration over all entries.

- The =cas= operation needed to be atomic with respect to other database
  operations issued by a different replica. It was achieved by *submitting all
  =cas=-related data as a single value* to Paxos. This mechanism is extended to
  provide transaction-style support without having to implement true database
  transactions.

- A primitive called =MultiOp= is used to implement all database functionality
  (except iteration). It consists of 3 components:

  1. A list of tests called *guard*. Each test in =guard= checks a single db
     entry. If all tests are true, =MultiOp= executes =t_op=, otherwise it
     executes =f_op=.

  2. A list of db operations called =t_op=. Each operation is either =insert=,
     =delete=, or =lookup=, and applies to a single db entry.

  3. A list of db operations called =f_op=, which is like =t_op=.

*** Software Engineering

**** Expressing the algorithm effectively

- They've coded the *core algorithm* as two state machines. They designed a
  simple statemachine specification language and built a compiler to translate
  such specifications into C++.

- Choosing a *specification language* makes it *easier to reason about and
  modify* our state machines than an explicitly coded implementation that is
  intermingled with the rest of the system.

  - They share their experience where they moved from 3 states to 2 states in
    the group membership algorithm. It took them about *one hour to make this
    change* and *three days to modify their tests* accordingly.

**** Runtime consistency checking

- The master periodically submits a *checksum request* to the database log. On
  receipt of this request, each replica computes a checksum of its local
  database (they used a shadow data structure to handle db operations
  concurrently).
- After the master completes a checksum computation, it sends its checksum to
  all replicas which *compare* the masterâ€™s checksum with their computed checksum.
- They had 3 db inconsistency incidents:
  - First was operator error
  - Second was probably due to hardware memory corruption
  - Third was probably due to illegal memory access from errant code.

**** Testing

- Tests have 2 modes: *safety* and *liveness*.

- Tests start in safety mode and *inject random failures* into the system. After
  running for a predetermined period of time, they stop injecting failures and
  give the system time to fully recover. Then they switch the test to liveness
  mode. The purpose for the liveness test is to verify that the system does not
  deadlock after a sequence of failures.

- Fault-tolerant systems try to *mask problems*. Thus, they can mask bugs or
  configuration problems while *lowering their own fault-tolerance*.
  - They tell the story of a typo in the configuration that result in a scenario
    where one of the five replicas always run in catch-up mode (which appears
    correct). This made the system tolerate only one faulty component (instead
    of two).

**** Concurrency

They claim that they set the "right goals" for repeatability of executions by
*constraining concurrency*. Unfortunately, as the product needs grew they were
unable to adhere to these goals, and added more concurrency support (to take
snapshots, compute checksums and process iterators while concurrently serving
database requests).

*** Unexpected Failures

These are the failures they've faced when they tried to move from the 3DB
version of Chubby to Paxos version during the course of *100 machine years*:

- First release shipped with ten times the number of worker threads as the
  original Chubby system. Under load, the worker threads ended up *starving*
  some other key threads and caused the system to time out frequently. This
  resulted in rapid master fail over, which resulted in a *cycle* of new master
  election, and master fail over. They've lost 15 hours of data.

- They had to address the changes in the difference in semantics. For example,
  if Chubby submitted an operation to the db, and the db lost its master status,
  Chubby expected the operation to fail. However, in the new system a replica
  could be re-installed as master, and operation could succeed. The fix took a
  substantial work and required the use of *epoch numbers* and =MultiOp=.

- They also had issues regarding to the OS: In Linux 2.4 kernel, when they try
  to flush a small file to disk, the call can hang for a long time if there are
  a lot of buffered writes to other files. The workaround was to write all large
  files in chunk.

- In total:
  - 3 failures occurred during upgrade (or rollback).
  - 2 were from bugs
  - 2 were due to operator errors. While they are very competent, they are
    usually not part of the development team that built the system, and
    therefore not familiar with its intricate details.
  - 1 was due to memory corruption

*** Measurements

- Tests ran two copies of Chubby on the same set of 5 servers (typical
  Pentium-class machines).
- Speed-up is between 1.2x to 4.4x

*** Open Problems

- There are *significant gaps* between the description of the Paxos algorithm
  and the needs of a real-world system. In order to build a real-world system,
  an expert needs to *use numerous ideas scattered in the literature* and make
  several relatively small protocol extensions. The cumulative effort will be
  substantial and the final system will be based on an *unproven protocol*.

- The fault-tolerance computing community *has not developed the tools* to make
  it easy to implement their algorithms.

- The fault-tolerance computing community *has not paid enough attention to
  testing*, a key ingredient for building fault-tolerant systems.

** Verdi: a framework for implementing and formally verifying distributed systems

[[http://verdi.uwplse.org/verdi.pdf][link]]

*** Introduction

- In April 2011 a malfunction of failure recovery in EC2 caused a major outage
  and brought down several web sites, including Foursquare, Reddit, Quora, and
  PBS.

- Few practical distributed system implementations have been formally verified.
  For performance reasons, real-world implementations often diverge in important
  ways from their high-level descriptions. So, goal is to *verify working code*.
  *Coq* is used to both write and verify the system.

- Distributed systems run in a diverse range of environments. Thus, it must support
  verifying applications against *different fault models*. It's specified as
  *network semantics*.

- Verdi aims to help the programmer separately prove correctness of application
  level behavior and correctness of fault tolerance mechanisms, and to allow
  these proofs to be easily *composed*.

  - They introduce *verified system transformers*, which is a function whose input
    is an implementation of a system and whose output is a *new system
    implementation* that makes different assumptions about its environment.

  - For example, a Verdi programmer can first build and verify a system assuming a
    reliable network, and then apply a transformer to obtain another version of
    their system that correctly and provably tolerates faults in an unreliable
    network.

- They implemented and verified
  - a key-value store,
  - a primary-backup replication transformer
  - first formally verified proof of linearizability for the Raft consensus protocol

- Verdi *decouples* the verification of application-level guarantees from the
  implementation and verification of fault-tolerance mechanisms.

- Verdi *DOESN'T SUPPORT* liveness properties *OR* capture Byzantine fault
  models.

*** Overview

- Programmer provides the specification, implementation (naming of nodes, IO for
  each node + msg that it responds to, state of each node, msg handling code
  that each node runs), and verification of this implementation that runs on a
  *idealized reliable model*.

- Then a target network semantics that reflects environment's fault model is
  selected, and a VST is applied to the implementation to transform it into one
  that is correct in that fault model. This transformation also produces updated
  versions of the specification and proof.

- The following are the steps taken to implement a lock service (one server):

**** Specification

- Correct behavior is specified in terms of *traces*
  
\begin{align*}
mutex(\tau) := & \tau = \tau_1 \mathtt{++}
\langle n_1, \mathtt{Grant} \rangle \mathtt{++}
\tau_2 \mathtt{++}
\langle n_2, \mathtt{Grant} \rangle
\mathtt{++} \tau_3 \\
& \rightarrow \langle n_1, \mathtt{Unlock} \rangle \in \tau_2
\end{align*}

**** Implementation
     
Names of the nodes in the system
#+BEGIN_SRC ocaml
Name := Server | Agent(int)
#+END_SRC

IO (external) defines the API of the service
#+BEGIN_SRC ocaml
Inp := Lock | Unlock
Out := Grant
(* 2 - network messages *)
Msg := LockMsg | UnlockMsg | GrantMsg
#+END_SRC

States of the nodes are defined and initialized
#+BEGIN_SRC ocaml
State (n: Name) :=
  match n with
  | Server => list Name (* head = agent holding lock *)
                        (* tail = agents waiting for lock *)
  | Agent n => bool     (* true iff this agent holds lock *)

InitState (n: Name) : State n :=
  match n with
  | Server => []
  | Agent n => false
#+END_SRC

Handlers for the external inputs are implemented
#+BEGIN_SRC ocaml
HandleInp (n: Name) (s: State n) (inp: Inp) :=
  match n with
  | Server => nop                 (* server performs no external IO *)
  | Agent n =>
    match inp with
    | Lock =>                     (* client requests lock *)
        send (Server, LockMsg)    (* forward to Server *)
    | Unlock =>                   (* client requests unlock *)
      if s == true then (         (* if lock held *)
        s := false;;              (* update state *)
        send (Server, UnlockMsg)) (* tell Server lock freed *)
#+END_SRC

Handlers for the network messages are implemented
#+BEGIN_SRC ocaml
HandleMsg (n: Name) (s: State n) (src: Name) (msg: Msg) :=
  match n with
  | Server =>
    match msg with
    | LockMsg =>
        (* if lock not held, immediately grant *)
        if s == [] then send (src, GrantMsg);;
        (* add requestor to end of queue *)
        s := s ++ [src]
    | UnlockMsg =>
      (* head of queue no longer holds lock *)
      s := tail s;;
      (* grant lock to next waiting agent, if any *)
      if s != [] then send (head s, GrantMsg)
    | _ => nop (* never happens *)
  | Agent n =>
    match msg with
    | GrantMsg =>               (* lock acquired *)
      s := true;;               (* update state *)
      output Grant              (* notify listeners *)
    | _ => nop                  (* never happens *)
#+END_SRC

- This implementation assumes that network is ideal
- Each node listens for input and message events and handles it accordingly.
- In a reliable network, each step of execution either 
  - picks an arbitrary node and delivers an arbitrary external input
  - picks a message in the network
  then runs the appropriate handler and updates the state

**** Verification

1. Prove an invariant about the *reachable node and network states* of the lock
   service application
2. *Relate* these reachable states to the *producible states*
3. Show that the previous two steps imply =mutex= holds on *all producible traces*
   
\begin{align*}
\mathtt{mutex}_{\mathtt{state}}(\Sigma, P) & := \forall n \; m, n \neq m \rightarrow
\neg \mathtt{haslock}(\Sigma,n) \vee \neg \mathtt{haslock}(\Sigma, m) \\

\mathtt{haslock}(\Sigma, n) & := \Sigma(\mathtt{Agent}(n)) = \mathtt{true}
\end{align*}

- The function $\Sigma$ maps node names to their state
- $P$ is the set of in-flight packets
- The property $\mathtt{mutex}_{\mathtt{state}}$ ensures that at most one agent
  node holds the lock at a time
- One *inductive state invariant* for $\mathtt{mutex}_{\mathtt{state}}$ is (~
  500 lines of Coq code) :

\begin{align*}
& ( \forall n, \; \mathtt{hasLock}(\Sigma, \;n) \rightarrow \mathtt{atHead}(\Sigma, \;n) ) 
&& \text{agree on who holds the lock} \\

\wedge & ( \forall p \in P, \; p.\mathtt{body} = \mathtt{GrantMsg} \rightarrow
\mathtt{grantee}(\Sigma, \; p.\mathtt{dest}) ) 
&& \text{grant is not given to holder of the lock} \\

\wedge & ( \forall p \in P, \; p.\mathtt{body} = \mathtt{UnlockMsg} \rightarrow
\mathtt{grantee}(\Sigma, \; p.\mathtt{source}) ) 
&& \text{unlock comes from the holder} \\

\wedge & \mathtt{at\_most\_one} \; \lbrace \mathtt{GrantMsg}, \; \mathtt{UnlockMsg}
\rbrace \; P 
&& \text{at most one in-flight message}
\end{align*}

where 

\begin{align*}
\mathtt{atHead}(\Sigma, \; n) \;\; & := \;\; \exists t, \;
\Sigma(\mathtt{Server}) = n :: t \\

\mathtt{grantee}(\Sigma, \; n) \;\; & := \;\; \mathtt{atHead}(\Sigma, \; n)
\wedge \neg \mathtt{hasLock}(\Sigma, \; n)
\end{align*}

- Second step of the proof relates reachable states to the traces that a system
  can produce. It requires that whenever a =Grant= output appears in the trace
  without a corresponding =Unlock= input, that agent is the current holder of
  the lock:

\begin{align*} 
\mathtt{trace\_state\_agreement}(\tau, \Sigma) & \; := \; \forall n, \;
\mathtt{lastGrant}(\tau, \; n) \leftrightarrow \mathtt{hasLock}(\Sigma, \; n) \\
\mathtt{lastGrant}(\tau, \; n) & \; := \; \tau = \tau_1 \mathtt{++} \langle n,
\mathtt{Grant} \rangle :: \tau_2 \; \wedge \; \forall m, \; \langle m, \mathtt{Unlock}
\rangle \not\in \tau_2
\end{align*}
**** Verified System Transformers

- To tolerate network faults (e.g. packet drops, duplication), they use
  *transmission transformers*.

- It provides a system transformer that adds sequence numbers to every outgoing
  packet and ignores the ones packets that it has already seen

- To tolerate node crashes, they use *replication transformers*

**** Running the lock service app
- Invoke Coqâ€™s built-in extraction mechanism to generate OCaml code from the Coq
  implementation, compile it with the OCaml compiler, and link it with a Verdi
  shim.

- The shim is written in OCaml; it implements network primitives (e.g., packet
  send/receive) and an event loop that invokes the appropriate event handler for
  incoming network packets, IO, or other events
*** Network Semantics

- Network semantics are defined as step relations on a "state of the world". The
  state of the world differs among network semantics, but always includes a
  trace of the systemâ€™s external input and output.

- *Single-node semantics*:
\[
\frac{H_{inp}(\sigma, i) = (\sigma', o)}{(\sigma, T) \rightarrow_{s} (\sigma', T
\mathtt{++} \langle i, o \rangle)} \text{INPUT}
\]
- *Reordering semantics*:

  $H_{inp}$ is for handling IO and $H_{net}$ is for handling network messages.


\begin{align*}  
\frac{H_{inp}(n, \Sigma[n], i) = (\sigma', o, P') \;\;\;\; \Sigma' = \Sigma[n \mapsto \sigma']
}{
(P, \Sigma, T) \rightarrow_{r} (P \uplus P', \Sigma', T \mathtt{++} \langle i, o \rangle)
} \text{INPUT}
\end{align*}  

\begin{align*}  
& \frac{
H_{net}(dst, \Sigma[dst], src, m) = (\sigma', o, P') \;\;\;\; \Sigma' = \Sigma[dst \mapsto \sigma']
}{
(\lbrace (src,dst,m)\rbrace \uplus P, \Sigma, T) \rightarrow_{r} (P \uplus P', \Sigma', T \mathtt{++} \langle o \rangle)
} \text{DELIVER}
\end{align*}

- *Duplicating semantics*:

\[
\frac{p \in P}{(P,\Sigma, T) \rightarrow_{\mathtt{dup}} (P \uplus \{p\}, \Sigma, T)} \text{DUPLICATE}
\]

- *Dropping semantics*:

  Handler functions only execute when packets are delivered and packets may be
  arbitrarily dropped. Systems which tolerate dropped packets need to
  re-transmit some messages, so the dropping semantics also includes a TIMEOUT
  rule

\[
\frac{}{(\{p\} \uplus P, \Sigma, T) \leftarrow_{\mathtt{drop}} (P, \Sigma, T)}
\text{DROP}
\]

\[
\frac{
H_{\mathtt{tmt}}(n, \Sigma[n]) = (\sigma', o, P') \;\;\;\; \Sigma'=\Sigma[n
\mapsto \sigma']
}{
(P, \Sigma, T) \rightarrow_{\mathtt{drop}} (P \uplus P', \Sigma', T \mathtt{++}
\langle \mathtt{tmt}, o \rangle )} \text{TIMEOUT}
\]

- *Node failure*:

  Set $F$ contains the failed nodes. A node can fail anytime, and failed nodes
  can return any time. When it returns, $H_{\mathtt{rbt}}$ function is run on
  its pre-failure state

\[
\frac{
n \not\in F
}{
(P, \Sigma, F, T) \rightarrow_{\mathtt{fail}} (P, \Sigma, F \cup \{n\}, T)
} \text{CRASH}
\]

\[
\frac{
n \not\in F \;\;\;\; H_{\mathtt{rbt}}(n, \Sigma[n]) = \sigma' \;\;\;\; \Sigma' =
\Sigma[n \mapsto \sigma']
}{
(P, \Sigma, F, T) \rightarrow_{\mathtt{fail}} (P, \Sigma', F - \{n\}, T)
} \text{REBOOT}
\]
*** Verified System Transformers

- System transformers are implemented as wrappers around the systemâ€™s state,
  messages, and handlers. 
- Messages and state are generally transformed to include additional fields.
- Handlers in the transformed system call into underlying handlers and implement
  additional functionality.
  
**** Sequence Numbering Transformer

- Sender uses a unique number for each of its message, receiver keeps a list of
  =<number, sender>= pairs they've seen.

- The following transformer =SeqNum= takes the a system =S= and describes a new
  one in reordering semantics

#+BEGIN_SRC ocaml
SeqNum (S) := 
  Name := S.Name
  
  Inp := S.Inp
  Out := S.Out 
  Msg := { seqnum: int; underlying_msg: S.Msg }

  State (n: Name) := { seen: list (Name * int); 
                       next_seqnum: int; 
                       underlying_state: S.State n }

  InitState (n: Name) := { seen := []; 
                           next_seqnum := 0; 
                           underlying_state := S.InitState n }

  HandleInp (n: Name) (s: State n) (inp: Inp) := 
    wrap_result (S.HandleInp (underlying_state s) inp)

  HandleMsg (n: Name) (s: State n) (src: Name) (msg: Msg) := 
    if not (contains s.seen (src, msg.seqnum)) then 
      s.seen := (src, msg.seqnum) :: s.seen;; 
      (* wrap_result adds sequence numbers to messages while 
         incrementing next_seqnum *) 
      wrap_result (S.HandleMsg n (underlying_state s) 
                               src (underlying_msg msg))
#+END_SRC

**** Correctness of Sequence Numbering

- Each verified transformer $T$ provides a function =transfer= which translates
  properties of traces in the underlying semantics $\rightarrow_1$ to the target
  semantics $\rightarrow_2$, where $\mathtt{holds}(\Phi, S, \rightarrow)$
  asserts that a property $\Phi$ is true of all traces of a system $S$ under the
  semantics defined by $\rightarrow$.

\[
\forall \Phi \; S, \;\mathtt{holds}(\Phi, S, \rightarrow_1) \rightarrow
\mathtt{holds}(\mathtt{transfer}(\Phi), T(S), \rightarrow_2)
\]

- For sequence numbering transformer, $\rightarrow_1$ is $\rightarrow_r$
  (reordering semantics) and $\rightarrow_2$ is $\rightarrow_{dup}$ (duplicating
  semantics).

- =transfer= function is the *identity*, since the properties of the traces are
  preserved.

- External output is depends only on the *wrapped state* of the system, and they
  prove that the wrapped state is preserved by *backward simulation*

- Backward simulation: For any step the transformed system $T(S)$ can take,
  $S$ can take an equivalent step.
  - =unwrap= returns the underlying state at each node
  - $\mathtt{dedup}_{\mathtt{net}}$ returns a bag of non-duplicate packets which
    haven't yet been delivered.

  The simulation is specified as the following, where $\rightarrow_{dup}^*$ and
  $\rightarrow_r^*$ are the reflexive transitive closures:

\[
(\Sigma_0, \emptyset, \emptyset) \; \rightarrow_{dup}^* \; (\Sigma, P, T) 
\;\;\;\; \rightarrow \;\;\;\;
(\mathtt{unwrap}(\Sigma_0), \emptyset, \emptyset) \; \rightarrow_{r}^* \;
(\mathtt{unwrap}(\Sigma), \mathtt{dedup}_{\mathtt{net}}(\Sigma, P), T)
\]

The proof is by induction on the step relation:

1. For =DUPLICATE= steps, $\rightarrow_{r}^*$ holds reflexively, since
   $\mathtt{dedup}_{\mathtt{net}}$ returns the same network when a packet is
   duplicated and the state and trace are unchanged. 
2. For =DELIVER= steps, the proof shows that either the delivered packet is
   ignored by the destination node, in which case
   $\mathtt{dedup}_{\mathtt{net}}$ holds reflexively, or that the underlying
   handler is run normally, in which case the underlying system can take the
   analogous DELIVER step. 
3. For both the =DELIVER= and =INPUT= steps, the proof shows that wrapping the
   sent packets results in a de-duplicated network that is reachable in the
   underlying system. 
  
*Ghost variables* are implemented as system transformers


*** Primary Backup Transformer (Case Study)

- Takes a single-node system and returns a replicated version of the system in
  the reordering semantics

- A primary node synchronously replicates requests to a backup node: when a
  request arrives, the primary ensures that the backup has processed it before
  applying it locally and replying to the client.

- They were able to show that either the primary and the backup have the same
  state or the backupâ€™s state is one step ahead of the primary.
  
*** Raft Replication Transformer (Case Study)
    
- They implement consistent state machine replication as a system transformer.
  It lifts a system designed for the state machine semantics into a system that
  tolerates machine crashes in the failure semantics.

- *Consensus transformer* provides an indistinguishability result for
  /linearizability/, which states that _any possible trace of the replicated
  system is equivalent to some valid trace of the underlying system_ under
  particular constraints about when operations can be re-ordered.

#+CAPTION: Raft's message type:
|----------+----------------------+----------------------|
|          | Name                 | Purpose              |
|----------+----------------------+----------------------|
| Messages | =AppendEntries=      | Log Replication      |
|          | =AppendEntriesReply= |                      |
|          | =RequestVote=        | Leader Election      |
|          | =RequestVoteReply=   |                      |
|----------+----------------------+----------------------|
| Inputs   | =ClientRequest=      | Client inputs        |
|----------+----------------------+----------------------|
| Outputs  | =ClientResponse=     | Successful execution |
|          | =NotLeader=          | Resubmit             |
|----------+----------------------+----------------------|

- Raft:
  - Divides time into /terms/ of arbitrary length
  - There can be *at most one leader* per term
  - If a node n suspects that leader is failed, it broadcasts =RequestVote=
  - If a quorum votes for it, then node n becomes the leader

**** Raft Proof

- *Linearizability* guarantees that clients see a consistent view of the state
  machine: clients see a consistent order in which operations were executed, and
  any request issued after a particular response is guaranteed to be ordered
  after that response.

- They've verified linearizability of *Raft's* states machine safety property,
  *not their* implementation of Raft.

- A trace \tau of requests I_1,...,I_n and responses O_1,...,O_m (where there is
  a total ordering on requests and responses) is linearizable with respect to an
  underlying state machine if there exists a trace of operations \tau' such
  that: 
  1. \tau' is a valid, sequential execution of the underlying state machine
     
     *Proof:* They show that the state machine state is correctly managed by Raft and
     that entries are applied in the order they appear in the log

  2. Every response in \tau has a corresponding operation in \tau'
     
     *Proof:* This follows from the fact Raft never issues a =ClientResponse=
     before the corresponding log entry is applied to the state machine

  3. If a response to an operation op_1 occurs before a request for an operation
     op_2 in \tau, then op_1 occurs before op_2 in \tau'.
     
     *Proof:* This holds because Raft only appends entries to the log: if a
     =ClientResponse= has already been issued, then that entry is already in the
     log, so any subsequent =ClientRequest= will be ordered after it in the log.

- The *key to the proof* is that Raftâ€™s internal log contains a linearized
  ordering of operations. The desired underlying trace, then, is just the list
  of operations in the order of the log.

#+CAPTION: Verification effort (# of lines)
| System                 | Spec | Impl | Proof |
|------------------------+------+------+-------|
| Sequence numbering     |   20 |   89 |   576 |
| Key-value store        |   41 |  138 |   337 |
| Primary-backup         |   20 |  134 |  1155 |
| KV+PB                  |    5 |  N/A |    19 |
| Raft (Linearizability) |  170 |  520 |  4144 |
| Verdi                  |  148 |  220 |  2364 |

*** Evaluation

- They applied the replication transformer to key-value store example, which
  they call =vard=.
- They show that Verdi has a comparable performance with respect to =etcd=, a
  production fault-tolerant key-value store written in the Go that uses Raft
  internally. Even though =vard= is not feature complete (=etcd= uses more
  complex network protocol, etc.).

*** Related work

- EventML: provides expressive primitives and combinators for implementing
  distributed systems. Programs can be automatically abstracted into formula in
  the /Logic of Events/, which can then be used to verify the system in /NuPRL/.
  
  ShadowDB uses this to implement a total-order broadcast service. However
  replicated database itself is not verified.

- HOL4 theorem prover: It is used to verify an implementation of the TCP and
  POSIX sockets API. In Verdi, rather than the network stack, they verify the
  high level application instead.

- Ensemble: Layers simple micro protocols to produce sophisticated distributed
  systems. Cross-protocol optimization is used to increase efficiency.

- Verified Software Defined Networking

- Specification reasoning: e.g. TLA

- Model checking & testing: e.g. Mace provides a full suite of tools for
  building and model checking distributed systems (including /liveness/
  properties)

* Unread
** Raft: In Search of an Understandable Consensus Algorithm
   
[[https://raft.github.io/raft.pdf][link]]

There's a simple explanation of the Raft algorithm (with visualizations) [[http://thesecretlivesofdata.com/raft/][here]].

** Ivy: Safety Verification by Interactive Generalization

[[https://www.cs.tau.ac.il/~odedp/ivy.pdf][link]]

** Verifying Fault-Tolerant Erlang Programs

[[ftp://ftp.inrialpes.fr/pub/vasy/publications/others/Benac-Fredlund-Derrick-05.pdf][link]]

** Analyzing Fault Tolerance for Erlang Applications

[[https://uu.diva-portal.org/smash/get/diva2:213697/FULLTEXT01.pdf][link]]

** Verifying fault-tolerant Erlang programs

[[https://www.researchgate.net/publication/221211408_Verifying_fault-tolerant_Erlang_programs][link]]

** SMT and POR beat Counter Abstraction: Parametrized Model Checking of Threshold-Based Distributed Algorithms

[[http://forsyte.at/download/konnov-cav15.pdf][link]]

** What You Always Wanted to Know About Model Checking of Fault-Tolerant Distributed Algorithms

[[https://www.researchgate.net/publication/304480515_What_You_Always_Wanted_to_Know_About_Model_Checking_of_Fault-Tolerant_Distributed_Algorithms][link]]

** Experience with Rules-Based Programming for Distributed, Concurrent, Fault-Tolerant Code

[[https://www.usenix.org/system/files/conference/atc15/atc15-paper-stutsman.pdf][link]]

** What's Decidable About Arrays?

[[http://www-step.stanford.edu/papers/avmcai06.pdf][link]]

** Counting Constraints in Flat Array Fragments

[[https://link.springer.com/chapter/10.1007/978-3-319-40229-1_6][link]]


